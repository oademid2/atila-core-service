{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMONs+84kJt66FMynCm9XHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oademid2/atila-core-service/blob/master/summarize_youtube_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize a Youtube Video"
      ],
      "metadata": {
        "id": "Tf6uuWKTV2Ik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpUqov3gVxJf"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence_transformers pytube\n",
        "\n",
        "# optional install pytorch so you can use a gpu for faster transcription\n",
        "# command below is for Linux. See instructions for mac and windows: https://pytorch.org/get-started/locally/\n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!apt install ffmpeg # https://stackoverflow.com/questions/51856340/how-to-install-package-ffmpeg-in-google-colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Dict\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import whisper\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import pytube\n",
        "import time\n",
        "\n",
        "\n",
        "class EndpointHandler():\n",
        "    # load the model\n",
        "    WHISPER_MODEL_NAME = \"tiny.en\"\n",
        "    SENTENCE_TRANSFORMER_MODEL_NAME = \"multi-qa-mpnet-base-dot-v1\"\n",
        "    QUESTION_ANSWER_MODEL_NAME = \"vblagoje/bart_lfqa\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def __init__(self, path=\"\"):\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f'whisper and question_answer_model will use: {device}')\n",
        "\n",
        "        t0 = time.time()\n",
        "        self.whisper_model = whisper.load_model(self.WHISPER_MODEL_NAME).to(device)\n",
        "        t1 = time.time()\n",
        "\n",
        "        total = t1 - t0\n",
        "        print(f'Finished loading whisper_model in {total} seconds')\n",
        "\n",
        "        t0 = time.time()\n",
        "        self.sentence_transformer_model = SentenceTransformer(self.SENTENCE_TRANSFORMER_MODEL_NAME)\n",
        "        t1 = time.time()\n",
        "\n",
        "        total = t1 - t0\n",
        "        print(f'Finished loading sentence_transformer_model in {total} seconds')\n",
        "        \n",
        "        self.question_answer_tokenizer = AutoTokenizer.from_pretrained(self.QUESTION_ANSWER_MODEL_NAME)\n",
        "        t0 = time.time()\n",
        "        self.question_answer_model = AutoModelForSeq2SeqLM.from_pretrained(self.QUESTION_ANSWER_MODEL_NAME).to(device)\n",
        "        t1 = time.time()\n",
        "        total = t1 - t0\n",
        "        print(f'Finished loading question_answer_model in {total} seconds')\n",
        "\n",
        "    def __call__(self, data: Dict[str, str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (:obj:):\n",
        "                includes the URL to video for transcription\n",
        "        Return:\n",
        "            A :obj:`dict`:. transcribed dict\n",
        "        \"\"\"\n",
        "        # process input\n",
        "        print('data', data)\n",
        "\n",
        "        if \"inputs\" not in data:\n",
        "            raise Exception(f\"data is missing 'inputs' key which  EndpointHandler expects. Received: {data}\"\n",
        "                            f\" See: https://huggingface.co/docs/inference-endpoints/guides/custom_handler#2-create-endpointhandler-cp\")\n",
        "        video_url = data.pop(\"video_url\", None)\n",
        "        query = data.pop(\"query\", None)\n",
        "        long_form_answer = data.pop(\"long_form_answer\", None)\n",
        "        encoded_segments = {}\n",
        "        if video_url:\n",
        "            video_with_transcript = self.transcribe_video(video_url)\n",
        "            video_with_transcript['transcript']['transcription_source'] = f\"whisper_{self.WHISPER_MODEL_NAME}\"\n",
        "            encode_transcript = data.pop(\"encode_transcript\", True)\n",
        "            if encode_transcript:\n",
        "                encoded_segments = self.combine_transcripts(video_with_transcript)\n",
        "                encoded_segments = {\n",
        "                    \"encoded_segments\": self.encode_sentences(encoded_segments)\n",
        "                }\n",
        "            return {\n",
        "                **video_with_transcript,\n",
        "                **encoded_segments\n",
        "            }\n",
        "        elif query:\n",
        "            if long_form_answer:\n",
        "                context = data.pop(\"context\", None)\n",
        "                answer = self.generate_answer(query, context)\n",
        "                response = {\n",
        "                    \"answer\": answer\n",
        "                }\n",
        "\n",
        "                return response\n",
        "            else:\n",
        "                query = [{\"text\": query, \"id\": \"\"}] if isinstance(query, str) else query\n",
        "                encoded_segments = self.encode_sentences(query)\n",
        "\n",
        "                response = {\n",
        "                    \"encoded_segments\": encoded_segments\n",
        "                }\n",
        "\n",
        "                return response\n",
        "\n",
        "        else:\n",
        "            return {\n",
        "                \"error\": \"'video_url' or 'query' must be provided\"\n",
        "            }\n",
        "\n",
        "    def summarize(self, video_url):\n",
        "        pass"
      ],
      "metadata": {
        "id": "KvoeDDKnV16s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}